<!DOCTYPE html>
<html lang="en">
<head>
    <meta charset="UTF-8">
    <meta name="viewport" content="width=device-width, initial-scale=1.0">
    <title>Session 3: ML Development and Algorithmic Bias - Course Content</title>
    <style>
        body {
            font-family: Arial, sans-serif;
            max-width: 1200px;
            margin: 0 auto;
            padding: 20px;
            line-height: 1.6;
            color: #333;
            background: linear-gradient(135deg, #034638 0%, #582C83 100%);
            min-height: 100vh;
        }
        
        .content-container {
            background: white;
            border-radius: 15px;
            padding: 40px;
            margin: 20px 0;
            box-shadow: 0 20px 40px rgba(0,0,0,0.15);
        }
        
        h1 {
            font-family: Georgia, serif;
            color: #034638;
            border-bottom: 3px solid #582C83;
            padding-bottom: 15px;
            font-size: 2.5em;
            margin-bottom: 20px;
        }
        
        h2 {
            font-family: Georgia, serif;
            color: #034638;
            margin-top: 40px;
            margin-bottom: 20px;
            font-size: 1.8em;
        }
        
        h3 {
            font-family: Georgia, serif;
            color: #582C83;
            margin-top: 30px;
            margin-bottom: 15px;
            font-size: 1.4em;
        }
        
        h4 {
            font-family: Georgia, serif;
            color: #63666A;
            margin-top: 25px;
            margin-bottom: 10px;
            font-size: 1.2em;
        }
        
        .header-info {
            background: #f0fdf4;
            border-left: 4px solid #034638;
            padding: 20px;
            border-radius: 8px;
            margin: 25px 0;
        }
        
        .objectives {
            background: #e7f4ea;
            padding: 25px;
            border-left: 6px solid #16a34a;
            margin: 30px 0;
            border-radius: 8px;
        }
        
        .section {
            background: #f9fafb;
            padding: 25px;
            margin: 25px 0;
            border-left: 4px solid #582C83;
            border-radius: 8px;
        }
        
        .compas-example {
            background: #fef3c7;
            padding: 20px;
            border-left: 4px solid #d97706;
            border-radius: 8px;
            margin: 20px 0;
            font-style: italic;
        }
        
        .business-insight {
            background: #e0f2fe;
            padding: 20px;
            border-left: 4px solid #034638;
            border-radius: 8px;
            margin: 20px 0;
        }
        
        .highlight {
            background-color: #fef3c7;
            padding: 3px 6px;
            border-radius: 3px;
        }
        
        ul li {
            margin-bottom: 10px;
        }
        
        ol li {
            margin-bottom: 12px;
        }
        
        .framework-box {
            background: linear-gradient(135deg, #f3f4f6 0%, #e5e7eb 100%);
            padding: 25px;
            border: 2px solid #034638;
            border-radius: 12px;
            margin: 30px 0;
        }
        
        .framework-box h3 {
            color: #034638;
            margin-top: 0;
        }
        
        .category-header {
            background: linear-gradient(135deg, #034638 0%, #582C83 100%);
            color: white;
            padding: 15px 20px;
            border-radius: 8px;
            margin: 25px 0 15px 0;
            font-weight: bold;
            font-size: 1.1em;
        }
        
        .bias-type {
            background: #f8fafc;
            border: 1px solid #e2e8f0;
            border-radius: 8px;
            padding: 20px;
            margin: 15px 0;
        }
        
        .bias-type h4 {
            color: #034638;
            margin-top: 0;
            margin-bottom: 10px;
            font-size: 1.3em;
        }
        
        .definition {
            color: #1f2937;
            font-style: italic;
            margin-bottom: 15px;
            font-size: 1.05em;
        }
        
        .fairness-comparison {
            display: grid;
            grid-template-columns: 1fr 1fr;
            gap: 30px;
            margin: 30px 0;
        }
        
        .position-box {
            background: white;
            border: 2px solid #e5e7eb;
            border-radius: 12px;
            padding: 25px;
        }
        
        .propublica {
            border-color: #dc2626;
            background: linear-gradient(135deg, #fef2f2 0%, #fee2e2 100%);
        }
        
        .northpointe {
            border-color: #2563eb;
            background: linear-gradient(135deg, #eff6ff 0%, #dbeafe 100%);
        }
        
        .position-box h3 {
            margin-top: 0;
            color: #034638;
        }
        
        .metrics-grid {
            display: grid;
            grid-template-columns: repeat(auto-fit, minmax(200px, 1fr));
            gap: 20px;
            margin: 25px 0;
        }
        
        .metric-card {
            background: white;
            border: 1px solid #d1d5db;
            border-radius: 8px;
            padding: 15px;
            text-align: center;
            box-shadow: 0 2px 4px rgba(0,0,0,0.1);
        }
        
        .metric-value {
            font-size: 2em;
            font-weight: bold;
            color: #034638;
            margin: 10px 0;
        }
        
        .stakeholder-perspectives {
            display: grid;
            grid-template-columns: repeat(auto-fit, minmax(280px, 1fr));
            gap: 25px;
            margin: 30px 0;
        }
        
        .stakeholder-box {
            background: #f8fafc;
            border: 2px solid #582C83;
            border-radius: 8px;
            padding: 20px;
        }
        
        .stakeholder-box h4 {
            color: #582C83;
            margin-top: 0;
        }
        
        .decision-framework {
            background: linear-gradient(135deg, #e7f4ea 0%, #d1f2d9 100%);
            border: 2px solid #16a34a;
            border-radius: 12px;
            padding: 25px;
            margin: 30px 0;
        }
        
        .decision-framework h3 {
            color: #16a34a;
            margin-top: 0;
        }
        
        .application-examples {
            display: grid;
            grid-template-columns: repeat(auto-fit, minmax(300px, 1fr));
            gap: 25px;
            margin: 30px 0;
        }
        
        .example-card {
            background: #f1f5f9;
            border: 2px solid #64748b;
            border-radius: 8px;
            padding: 20px;
        }
        
        .example-card h4 {
            color: #334155;
            margin-top: 0;
        }
        
        .strategy-list {
            background: #fef7ff;
            border: 2px solid #c084fc;
            border-radius: 8px;
            padding: 20px;
            margin: 20px 0;
        }
        
        .conclusion-box {
            background: linear-gradient(135deg, #034638 0%, #582C83 100%);
            color: white;
            padding: 30px;
            border-radius: 15px;
            margin: 40px 0;
            text-align: center;
        }
        
        .conclusion-box h2 {
            color: white;
            margin-top: 0;
        }
        
        .call-to-action {
            font-size: 1.1em;
            font-weight: bold;
            margin-top: 20px;
        }
        
        .reflection-questions {
            background: #f0fdf4;
            border: 2px solid #16a34a;
            border-radius: 12px;
            padding: 25px;
            margin: 30px 0;
        }
        
        .reflection-questions h3 {
            color: #16a34a;
            margin-top: 0;
        }
        
        .flow-chart {
            background: #f8fafc;
            border: 2px solid #64748b;
            border-radius: 12px;
            padding: 30px;
            margin: 30px 0;
            font-family: monospace;
            font-size: 1.1em;
        }
        
        .key-stages {
            display: grid;
            grid-template-columns: repeat(auto-fit, minmax(280px, 1fr));
            gap: 20px;
            margin: 25px 0;
        }
        
        .stage-card {
            background: white;
            border: 2px solid #e5e7eb;
            border-radius: 8px;
            padding: 20px;
            transition: all 0.3s ease;
        }
        
        .stage-card:hover {
            border-color: #034638;
            box-shadow: 0 4px 8px rgba(0,0,0,0.1);
        }
        
        .stage-number {
            background: #034638;
            color: white;
            width: 30px;
            height: 30px;
            border-radius: 50%;
            display: flex;
            align-items: center;
            justify-content: center;
            font-weight: bold;
            margin-bottom: 15px;
        }
        
        .stage-title {
            color: #034638;
            font-weight: bold;
            margin-bottom: 10px;
            font-size: 1.1em;
        }
        
        .simulator-placeholder {
            background: linear-gradient(135deg, #f0fdf4 0%, #e7f4ea 100%);
            border: 3px dashed #034638;
            border-radius: 12px;
            padding: 30px;
            margin: 30px 0;
            text-align: center;
            color: #034638;
        }
        
        .simulator-placeholder h3 {
            color: #034638;
            margin-top: 0;
            margin-bottom: 15px;
            font-size: 1.6em;
        }
        
        .simulator-placeholder p {
            font-size: 1.1em;
            margin-bottom: 10px;
        }
        
        .simulator-filename {
            font-family: monospace;
            background: #f3f4f6;
            padding: 5px 10px;
            border-radius: 4px;
            color: #1f2937;
            font-weight: bold;
        }
    </style>
</head>
<body>
    <div class="content-container">
        <h1>Session 3: ML Development and Algorithmic Bias</h1>
        <p style="font-style: italic; font-size: 1.2em; color: #63666A; text-align: center; margin-bottom: 30px;">
            Foundational Course Content
        </p>

        <div class="header-info">
            <strong>Session Focus:</strong> Understanding how machine learning systems are built and where bias enters the development process<br>
            <strong>Case Study:</strong> COMPAS Recidivism Prediction Algorithm<br>
            <strong>Key Framework:</strong> 6-Stage ML Development Cycle + 9 Types of Bias + Competing Fairness Definitions
        </div>

        <div class="objectives">
            <h2>Why Understanding ML Bias Matters for Business Leaders</h2>
            <p>In 2016, a ProPublica investigation revealed that a widely-used criminal justice algorithm called COMPAS was producing racially biased predictions. This wasn't just a technical problem—it was a business and societal crisis that highlighted how algorithmic decisions can perpetuate systemic inequalities at scale.</p>
            <p>As a business leader in the AI era, you'll face similar challenges: How do you ensure your AI systems are fair? How do you balance competing definitions of fairness? How do you identify bias sources before they become public relations disasters or legal liabilities?</p>
            <p><strong>This session equips you with a systematic framework for identifying, classifying, and addressing bias throughout the machine learning development lifecycle.</strong></p>
        </div>

        <div class="section">
            <h2>Section 1: The Machine Learning Development Cycle</h2>
            
            <h3>Understanding How AI Systems Are Built</h3>
            <p>Before we can identify where bias emerges, we need to understand how machine learning systems are developed. Every ML system, from recommendation algorithms to predictive analytics, follows a similar six-stage lifecycle:</p>
            
            <div class="key-stages">
                <div class="stage-card">
                    <div class="stage-number">1</div>
                    <div class="stage-title">Data Collection</div>
                    <div><strong>What happens:</strong> Teams gather data from the target population to train the algorithm.</div>
                    <div><strong>Key decisions:</strong> What data to collect, from whom, and how to measure relevant outcomes.</div>
                    <div><strong>Business impact:</strong> These choices determine what the system can and cannot learn.</div>
                </div>

                <div class="stage-card">
                    <div class="stage-number">2</div>
                    <div class="stage-title">Data Preparation</div>
                    <div><strong>What happens:</strong> Raw data is cleaned, processed, and structured for machine learning.</div>
                    <div><strong>Key decisions:</strong> How to handle missing values, which variables to include, how to split data for training and testing.</div>
                    <div><strong>Business impact:</strong> Processing choices can amplify or introduce new biases.</div>
                </div>

                <div class="stage-card">
                    <div class="stage-number">3</div>
                    <div class="stage-title">Model Development</div>
                    <div><strong>What happens:</strong> The algorithm learns patterns from the training data to make predictions.</div>
                    <div><strong>Key decisions:</strong> Which algorithm to use, what to optimize for, how complex to make the model.</div>
                    <div><strong>Business impact:</strong> The objective function determines what the system prioritizes.</div>
                </div>

                <div class="stage-card">
                    <div class="stage-number">4</div>
                    <div class="stage-title">Model Evaluation</div>
                    <div><strong>What happens:</strong> The trained model's performance is assessed using test data.</div>
                    <div><strong>Key decisions:</strong> Which metrics to evaluate, what constitutes "good enough" performance.</div>
                    <div><strong>Business impact:</strong> Limited evaluation can mask bias or unfairness.</div>
                </div>

                <div class="stage-card">
                    <div class="stage-number">5</div>
                    <div class="stage-title">Post-processing</div>
                    <div><strong>What happens:</strong> Model outputs are transformed into formats suitable for end users.</div>
                    <div><strong>Key decisions:</strong> How to translate probability scores into categories, where to set decision thresholds.</div>
                    <div><strong>Business impact:</strong> These choices directly affect who gets classified as "high risk" or "low risk."</div>
                </div>

                <div class="stage-card">
                    <div class="stage-number">6</div>
                    <div class="stage-title">Deployment</div>
                    <div><strong>What happens:</strong> The system is implemented in real-world settings with actual users.</div>
                    <div><strong>Key decisions:</strong> How to integrate with existing workflows, how to train users, how to monitor performance.</div>
                    <div><strong>Business impact:</strong> Real-world usage often differs from intended design, creating new bias sources.</div>
                </div>
            </div>

            <div class="compas-example">
                <strong>COMPAS Throughout the Development Cycle:</strong><br>
                <strong>Data Collection:</strong> Northpointe collected historical arrest records, demographic information, and questionnaire responses from defendants across multiple jurisdictions.<br>
                <strong>Data Preparation:</strong> Missing employment history was filled using statistical methods; continuous variables like age were normalized; responses were coded into numerical scales.<br>
                <strong>Model Development:</strong> The team chose optimization approaches that prioritized overall accuracy, potentially at the cost of fairness across racial groups.<br>
                <strong>Model Evaluation:</strong> Initial evaluations focused on overall accuracy rates rather than examining performance differences across demographic groups.<br>
                <strong>Post-processing:</strong> Continuous risk scores (1-10) were grouped into low, medium, and high-risk categories, with thresholds that affected different racial groups differently.<br>
                <strong>Deployment:</strong> Different judges used COMPAS scores differently—some relied heavily on them, others barely considered them, creating inconsistent implementation.
            </div>

            <div class="framework-box">
                <h3>🔄 The Bias Cascade Effect</h3>
                <p><strong>Critical Insight:</strong> Bias doesn't stay contained within one stage. Small biases in data collection can be amplified during model development and further distorted during deployment, creating a "cascade effect" that magnifies unfairness throughout the system.</p>
            </div>
            
            <!-- SIMULATOR INSERTION POINT 1 -->
            <div class="simulator-placeholder">
                <h3>🔧 Interactive Exercise: Explore the ML Development Pipeline</h3>
                <p><strong>Now that you understand the six stages conceptually, let's see bias in action.</strong></p>
                
                <p>This simulator lets you:</p>
                <ul style="text-align: left; margin: 20px auto; max-width: 600px;">
                    <li>🎛️ <strong>Choose scenarios:</strong> COMPAS, Hiring AI, Medical Diagnosis, or Credit Scoring</li>
                    <li>🎯 <strong>Click stage cards:</strong> Select any of the 6 development stages to focus on</li>
                    <li>⚠️ <strong>Add bias sources:</strong> Inject bias at selected stages and watch the red indicators appear</li>
                    <li>📈 <strong>Watch the cascade:</strong> See how bias flows through the pipeline and compounds</li>
                    <li>📊 <strong>Compare demographics:</strong> Observe growing performance gaps between groups</li>
                    <li>🔄 <strong>Reset and experiment:</strong> Try different bias combinations</li>
                </ul>
                
                <div style="background: #eff6ff; border: 2px solid #034638; border-radius: 8px; padding: 15px; margin: 20px 0;">
                    <strong>Key Learning Goal:</strong> Understanding that bias isn't just a "data problem" - it can enter at ANY stage and gets worse as it moves through the pipeline. Small biases in data collection become major fairness problems by deployment.
                </div>
                
                <p><strong>Try this experiment:</strong> Start with "Clean Data" and add bias sources one stage at a time. Notice how the bias thermometer fills up red and demographic performance gaps widen with each addition.</p>
                
                <p><strong>File:</strong> <span class="simulator-filename">Session_3_ML_Development_Simulator_Ivey_Brand.html</span></p>
            </div>
        </div>

        <div class="section">
            <h2>Section 2: Nine Types of Machine Learning Bias</h2>
            
            <h3>A Systematic Framework for Bias Identification</h3>
            <p>Research has identified nine distinct types of bias that can emerge during ML development. These fall into three interconnected categories that create a "vicious cycle" of bias reinforcement.</p>
            
            <div class="category-header">
                Category 1: Data-to-Algorithm Bias
                <div style="font-size: 0.9em; font-weight: normal; margin-top: 5px;">How data problems create algorithmic unfairness</div>
            </div>

            <div class="bias-type">
                <h4>1. Measurement Bias</h4>
                <div class="definition">Systematic inaccuracies in how features and outcomes are measured or defined.</div>
                <p><strong>How it emerges:</strong> When we use proxies (substitute measures) that don't equally represent the construct we're trying to measure across all groups.</p>
                
                <div class="compas-example">
                    <strong>COMPAS Example:</strong>
                    <ul>
                        <li><strong>Arrest as proxy for crime:</strong> Police practices and racial profiling mean arrest rates don't equally reflect actual criminal behavior across racial groups</li>
                        <li><strong>Rearrest as proxy for recidivism:</strong> Rearrest depends on police practices, not just reoffending behavior</li>
                    </ul>
                </div>
                
                <div class="business-insight">
                    <strong>Business Impact:</strong> Teams often use easily available data (clicks, purchases, applications) as proxies for harder-to-measure concepts (engagement, satisfaction, qualifications), creating systematic measurement errors.
                </div>
            </div>

            <div class="bias-type">
                <h4>2. Representation Bias</h4>
                <div class="definition">When the training data doesn't adequately represent the population the model will serve.</div>
                <p><strong>How it emerges:</strong> Through non-representative sampling, missing subgroups, or implicit correlations that exclude certain communities.</p>
                
                <div class="compas-example">
                    <strong>COMPAS Example:</strong>
                    <ul>
                        <li>Training data overrepresented Black defendants due to historical over-policing</li>
                        <li>Socioeconomic factors correlated with race weren't adequately separated</li>
                        <li>Questions about friends' drug use and parental incarceration disproportionately affected certain communities</li>
                    </ul>
                </div>
                
                <div class="business-insight">
                    <strong>Business Impact:</strong> If your training data doesn't reflect your actual customer base, the model will perform poorly for underrepresented groups—potentially your growth markets.
                </div>
            </div>

            <div class="bias-type">
                <h4>3. Aggregation Bias</h4>
                <div class="definition">Using a single model for populations that should be treated differently.</div>
                <p><strong>How it emerges:</strong> When we assume relationships between variables are consistent across all subgroups.</p>
                
                <div class="compas-example">
                    <strong>COMPAS Example:</strong> A single risk model was applied to all defendants, despite evidence that factors affecting recidivism might vary by community context, socioeconomic status, or other demographic factors.
                </div>
                
                <div class="business-insight">
                    <strong>Business Impact:</strong> One-size-fits-all models often work well for majority groups but fail minorities, creating performance gaps that compound over time.
                </div>
            </div>

            <div class="category-header">
                Category 2: Algorithm-to-Users Bias
                <div style="font-size: 0.9em; font-weight: normal; margin-top: 5px;">How algorithmic choices and interfaces create unfairness</div>
            </div>

            <div class="bias-type">
                <h4>4. Learning Bias</h4>
                <div class="definition">When algorithmic design choices amplify performance disparities across groups.</div>
                <p><strong>How it emerges:</strong> Through objective function choices, optimization approaches, or model complexity decisions that prioritize some groups over others.</p>
                
                <div class="compas-example">
                    <strong>COMPAS Example:</strong>
                    <ul>
                        <li>Optimizing for overall accuracy led to better performance on the racial majority group</li>
                        <li>Simple linear models may have been insufficient for the complexity of different community contexts</li>
                        <li>The algorithm learned to be more "cautious" (more false positives) for groups with higher base rates</li>
                    </ul>
                </div>
                
                <div class="business-insight">
                    <strong>Business Impact:</strong> Technical optimization choices have business consequences—what you optimize for determines who benefits from your AI system.
                </div>
            </div>

            <div class="bias-type">
                <h4>5. User-Interaction Bias</h4>
                <div class="definition">How the AI system's interface and presentation affect user behavior and decisions.</div>
                <p><strong>How it emerges:</strong> Through interface design, information presentation, ranking systems, or decision support formats.</p>
                
                <div class="compas-example">
                    <strong>COMPAS Example:</strong>
                    <ul>
                        <li>Presenting risk as binary (high/low) rather than probabilistic created more polarized decision-making</li>
                        <li>Judges varied in how they interpreted and used risk scores</li>
                        <li>The interface didn't explain uncertainty or confidence levels</li>
                    </ul>
                </div>
                
                <div class="business-insight">
                    <strong>Business Impact:</strong> How you present AI insights affects how people use them. Poor interfaces can amplify bias even when the underlying algorithm is fair.
                </div>
            </div>

            <div class="bias-type">
                <h4>6. Evaluation Bias</h4>
                <div class="definition">When performance metrics and benchmarks don't adequately represent real-world impact or diverse populations.</div>
                <p><strong>How it emerges:</strong> Through limited evaluation metrics, non-representative test data, or inadequate external validation.</p>
                
                <div class="compas-example">
                    <strong>COMPAS Example:</strong>
                    <ul>
                        <li>Early evaluations focused on overall accuracy rather than fairness metrics</li>
                        <li>Limited testing on diverse populations</li>
                        <li>Lack of transparency prevented external validation</li>
                        <li>No long-term monitoring of differential performance</li>
                    </ul>
                </div>
                
                <div class="business-insight">
                    <strong>Business Impact:</strong> "What gets measured gets managed." If your evaluation doesn't include fairness metrics, bias problems will go undetected until they become crises.
                </div>
            </div>

            <div class="category-header">
                Category 3: Users-to-Data Bias
                <div style="font-size: 0.9em; font-weight: normal; margin-top: 5px;">How user behavior creates feedback loops that perpetuate bias</div>
            </div>

            <div class="bias-type">
                <h4>7. Historical Bias</h4>
                <div class="definition">When training data reflects past discrimination and systemic inequalities.</div>
                <p><strong>How it emerges:</strong> Using historical data that embeds societal biases as training material for future decisions.</p>
                
                <div class="compas-example">
                    <strong>COMPAS Example:</strong>
                    <ul>
                        <li>Training on historical criminal justice data that reflected decades of discriminatory policing practices</li>
                        <li>Arrest records encoded patterns of over-policing in minority communities</li>
                        <li>Historical sentencing disparities were baked into the training data</li>
                    </ul>
                </div>
                
                <div class="business-insight">
                    <strong>Business Impact:</strong> Your historical data may reflect past discrimination in hiring, lending, marketing, or service provision. Using it uncritically perpetuates these patterns.
                </div>
            </div>

            <div class="bias-type">
                <h4>8. Deployment Bias (Behavioral Bias)</h4>
                <div class="definition">Mismatch between how a system was designed to be used and how it's actually used in practice.</div>
                <p><strong>How it emerges:</strong> Through user interpretation differences, system misuse, automation bias, or confirmation bias.</p>
                
                <div class="compas-example">
                    <strong>COMPAS Example:</strong>
                    <ul>
                        <li>Some judges used COMPAS as primary evidence, others barely considered it</li>
                        <li>Intended for resource allocation but used for sentencing decisions</li>
                        <li>Judges' existing biases affected how they interpreted risk scores</li>
                        <li>Inconsistent application across different courtrooms and jurisdictions</li>
                    </ul>
                </div>
                
                <div class="business-insight">
                    <strong>Business Impact:</strong> Even unbiased algorithms can create unfair outcomes if deployed inconsistently or used beyond their intended purpose.
                </div>
            </div>

            <div class="bias-type">
                <h4>9. Self-Selection Bias</h4>
                <div class="definition">When the people included in your data set aren't representative because of voluntary participation or systematic exclusion.</div>
                <p><strong>How it emerges:</strong> Through voluntary participation, behavioral requirements for data collection, or social/economic barriers to participation.</p>
                
                <div class="compas-example">
                    <strong>COMPAS Example:</strong>
                    <ul>
                        <li>Defendants might provide answers designed to make them appear less risky</li>
                        <li>Survey responses reflected who was willing and able to participate fully</li>
                        <li>Language barriers, literacy levels, or cultural differences affected participation</li>
                    </ul>
                </div>
                
                <div class="business-insight">
                    <strong>Business Impact:</strong> Who chooses to interact with your AI system affects what it learns. Self-selection can systematically exclude perspectives you need to understand.
                </div>
            </div>

            <div class="framework-box">
                <h3>🔄 The Vicious Cycle: How Bias Categories Reinforce Each Other</h3>
                <div class="flow-chart">
                    <strong>Critical Understanding:</strong> These nine bias types don't exist in isolation. They create reinforcing feedback loops:<br><br>
                    
                    <strong>Historical bias</strong> (biased training data) →<br>
                    <strong>Measurement bias</strong> (problematic proxies) →<br>
                    <strong>Learning bias</strong> (algorithmic amplification) →<br>
                    <strong>Deployment bias</strong> (biased usage) →<br>
                    <strong>New biased data</strong> →<br>
                    <strong>Reinforced historical bias</strong><br><br>
                    
                    This cycle means that small initial biases can compound into significant systematic unfairness over time.
                </div>
            </div>
            
            <!-- SIMULATOR INSERTION POINT 2 -->
            <div class="simulator-placeholder">
                <h3>👥→👤 Interactive Exercise: Group-to-Individual Problem</h3>
                <p>Insert the Group-to-Individual Problem Simulator here to let students experience Professor Cyprian's height analogy.</p>
                <p><strong>File:</strong> <span class="simulator-filename">Session_3_Group_Individual_Simulator_Ivey_Brand.html</span></p>
                <p><em>Students will make predictions across different scenarios and discover why individual predictions often fail.</em></p>
            </div>
        </div>

        <div class="section">
            <h2>Section 3: Competing Definitions of Fairness</h2>
            
            <h3>The Heart of the COMPAS Debate</h3>
            <p>The COMPAS controversy wasn't just about whether the algorithm was biased—it was about fundamentally different definitions of what "fairness" means in algorithmic decision-making. Understanding these competing definitions is crucial for business leaders who must make similar choices.</p>
            
            <div class="fairness-comparison">
                <div class="position-box propublica">
                    <h3>ProPublica's Position: Fairness in Outcomes (Error Rate Balance)</h3>
                    <p><strong>Core Argument:</strong> A fair algorithm should make similar types of mistakes for all racial groups.</p>
                    
                    <p><strong>Key Metrics:</strong></p>
                    <ul>
                        <li><strong>False Positive Rate (FPR):</strong> Among people who won't reoffend, what percentage does the algorithm incorrectly label as high-risk?</li>
                        <li><strong>False Negative Rate (FNR):</strong> Among people who will reoffend, what percentage does the algorithm incorrectly label as low-risk?</li>
                    </ul>
                    
                    <div class="metrics-grid">
                        <div class="metric-card">
                            <div>Black Defendants</div>
                            <div class="metric-value">44.9%</div>
                            <div>False Positive Rate</div>
                        </div>
                        <div class="metric-card">
                            <div>White Defendants</div>
                            <div class="metric-value">23.5%</div>
                            <div>False Positive Rate</div>
                        </div>
                        <div class="metric-card">
                            <div>White Defendants</div>
                            <div class="metric-value">47.7%</div>
                            <div>False Negative Rate</div>
                        </div>
                        <div class="metric-card">
                            <div>Black Defendants</div>
                            <div class="metric-value">28.0%</div>
                            <div>False Negative Rate</div>
                        </div>
                    </div>
                    
                    <div class="business-insight">
                        <strong>The Fairness Principle:</strong> "If you're not going to reoffend, you should have the same chance of being wrongly labeled high-risk regardless of your race."<br><br>
                        <strong>Business Translation:</strong> This approach prioritizes equal treatment—ensuring that the algorithm's errors don't systematically disadvantage any group.
                    </div>
                </div>

                <div class="position-box northpointe">
                    <h3>Northpointe's Position: Fairness in Procedure (Predictive Parity)</h3>
                    <p><strong>Core Argument:</strong> A fair algorithm should be equally accurate in its predictions for all racial groups.</p>
                    
                    <p><strong>Key Metric:</strong></p>
                    <ul>
                        <li><strong>Positive Predictive Value (PPV):</strong> Among people labeled high-risk, what percentage actually reoffend?</li>
                    </ul>
                    
                    <div class="metrics-grid">
                        <div class="metric-card">
                            <div>Black Defendants</div>
                            <div class="metric-value">63.0%</div>
                            <div>Actually Reoffended</div>
                        </div>
                        <div class="metric-card">
                            <div>White Defendants</div>
                            <div class="metric-value">59.0%</div>
                            <div>Actually Reoffended</div>
                        </div>
                    </div>
                    
                    <div class="business-insight">
                        <strong>The Fairness Principle:</strong> "If you're labeled high-risk, your actual likelihood of reoffending should be similar regardless of your race."<br><br>
                        <strong>Business Translation:</strong> This approach prioritizes equal accuracy—ensuring that the algorithm's predictions mean the same thing across different groups.
                    </div>
                </div>
            </div>

            <div class="framework-box">
                <h3>⚖️ Why Both Can't Be True Simultaneously: The Mathematical Trade-off</h3>
                <p><strong>The Statistical Reality:</strong> When groups have different base rates (different underlying rates of the outcome you're predicting), you cannot simultaneously achieve both error rate balance AND predictive parity.</p>
                
                <div class="metrics-grid">
                    <div class="metric-card">
                        <div>Black Defendants</div>
                        <div class="metric-value">51%</div>
                        <div>Recidivism Rate</div>
                    </div>
                    <div class="metric-card">
                        <div>White Defendants</div>
                        <div class="metric-value">39%</div>
                        <div>Recidivism Rate</div>
                    </div>
                </div>
                
                <p><strong>The Mathematical Constraint:</strong> If Group A has a higher base rate than Group B, optimizing for one fairness definition will create disparities in the other.</p>
                <p><strong>Business Implication:</strong> You must choose which type of fairness to prioritize. This is fundamentally a values decision, not a technical one.</p>
            </div>
            
            <!-- SIMULATOR INSERTION POINT 3 -->
            <div class="simulator-placeholder">
                <h3>⚖️ Interactive Exercise: COMPAS Fairness Trade-offs</h3>
                <p>Insert the COMPAS Fairness Simulator here to let students manipulate data and explore competing fairness definitions.</p>
                <p><strong>File:</strong> <span class="simulator-filename">Session_3_Fairness_Simulator_Ivey_Brand.html</span></p>
                <p><em>Students will adjust sliders to see how optimizing for one fairness metric affects others.</em></p>
            </div>

            <h3>Stakeholder Perspectives on Fairness</h3>
            
            <div class="stakeholder-perspectives">
                <div class="stakeholder-box">
                    <h4>👨‍⚖️ From a Judge's Perspective</h4>
                    <p><strong>Predictive Parity Focus:</strong> "I need to know that when the algorithm says someone is high-risk, they actually are high-risk, regardless of their background."</p>
                    <p><strong>Concerns:</strong> Consistency in decision-making, effective resource allocation, public safety</p>
                </div>

                <div class="stakeholder-box">
                    <h4>⚖️ From a Defendant's Perspective</h4>
                    <p><strong>Error Rate Balance Focus:</strong> "I shouldn't be more likely to be wrongly labeled as dangerous just because of my demographic group."</p>
                    <p><strong>Concerns:</strong> Individual fairness, equal treatment under law, avoiding discrimination</p>
                </div>

                <div class="stakeholder-box">
                    <h4>🏛️ From a Society's Perspective</h4>
                    <p><strong>Complex Trade-offs:</strong> Balancing public safety, individual rights, resource efficiency, and systemic equality</p>
                    <p><strong>Concerns:</strong> Long-term social cohesion, trust in institutions, addressing historical inequities</p>
                </div>
            </div>

            <div class="decision-framework">
                <h3>🎯 Business Decision Framework: Choosing Your Fairness Definition</h3>
                <p>When implementing AI systems, business leaders must explicitly choose their fairness approach:</p>
                
                <p><strong>Questions to Ask:</strong></p>
                <ol>
                    <li><strong>What are the stakes?</strong> High-stakes decisions (hiring, lending, criminal justice) may require different fairness approaches than lower-stakes ones (content recommendations).</li>
                    <li><strong>Who are your stakeholders?</strong> Different groups may prioritize different fairness definitions.</li>
                    <li><strong>What are the base rates?</strong> If outcome rates differ significantly across groups, you'll face trade-offs.</li>
                    <li><strong>What are your values?</strong> Do you prioritize equal treatment or equal outcomes? Individual fairness or group fairness?</li>
                    <li><strong>What are the legal requirements?</strong> Some jurisdictions have specific requirements for algorithmic fairness.</li>
                </ol>
                
                <div class="strategy-list">
                    <strong>Strategic Recommendations:</strong>
                    <ul>
                        <li><strong>Be explicit:</strong> Don't pretend there's only one definition of fairness</li>
                        <li><strong>Involve stakeholders:</strong> Let affected communities participate in defining fairness</li>
                        <li><strong>Monitor multiple metrics:</strong> Track both error rates and predictive accuracy</li>
                        <li><strong>Plan for trade-offs:</strong> Acknowledge that perfect fairness across all definitions is mathematically impossible</li>
                        <li><strong>Maintain transparency:</strong> Explain your fairness choices and their implications</li>
                    </ul>
                </div>
            </div>
        </div>

        <div class="section">
            <h2>Section 4: Applying the Framework to Real-World AI Systems</h2>
            
            <h3>Beyond COMPAS: Recognizing Bias in Business Contexts</h3>
            <p>The same bias patterns that affected COMPAS appear across business applications of AI. Here's how to apply this framework to common organizational contexts:</p>
            
            <div class="application-examples">
                <div class="example-card">
                    <h4>💼 Hiring and Recruitment AI</h4>
                    <ul>
                        <li><strong>Measurement bias:</strong> Resume screening AI using past hiring patterns as training data</li>
                        <li><strong>Historical bias:</strong> If past hiring was biased, training data embeds that discrimination</li>
                        <li><strong>Representation bias:</strong> If certain groups are underrepresented in training data</li>
                        <li><strong>Deployment bias:</strong> Recruiters might use AI recommendations differently for different candidate pools</li>
                    </ul>
                </div>

                <div class="example-card">
                    <h4>💳 Credit Scoring and Financial Services</h4>
                    <ul>
                        <li><strong>Measurement bias:</strong> Using zip code, shopping patterns, or social media as proxies for creditworthiness</li>
                        <li><strong>Aggregation bias:</strong> Single model applied across different socioeconomic contexts</li>
                        <li><strong>Self-selection bias:</strong> Who applies for credit affects what the model learns</li>
                    </ul>
                </div>

                <div class="example-card">
                    <h4>📈 Marketing and Customer Targeting</h4>
                    <ul>
                        <li><strong>Representation bias:</strong> Training on existing customer base might miss growth opportunities</li>
                        <li><strong>User-interaction bias:</strong> How targeting recommendations are presented affects marketer behavior</li>
                        <li><strong>Evaluation bias:</strong> Focusing only on immediate conversion rather than long-term customer value</li>
                    </ul>
                </div>
            </div>

            <h3>Organizational Strategies for Bias Prevention</h3>
            
            <div class="strategy-list">
                <h4>1. Build Diverse, Cross-Functional Teams</h4>
                <ul>
                    <li>Include ethicists, community representatives, and domain experts in AI development</li>
                    <li>Ensure decision-makers understand the social context of their technical choices</li>
                </ul>
                
                <h4>2. Implement Bias Testing Throughout Development</h4>
                <ul>
                    <li><strong>Data stage:</strong> Audit training data for representation gaps and measurement issues</li>
                    <li><strong>Model stage:</strong> Test performance across different demographic groups</li>
                    <li><strong>Deployment stage:</strong> Monitor real-world performance and user behavior</li>
                </ul>
                
                <h4>3. Create Accountability Mechanisms</h4>
                <ul>
                    <li>Designate responsible parties for fairness outcomes</li>
                    <li>Establish regular bias audits and reviews</li>
                    <li>Create channels for stakeholder feedback and complaints</li>
                </ul>
                
                <h4>4. Plan for Continuous Monitoring</h4>
                <ul>
                    <li>AI bias isn't a one-time fix—it requires ongoing attention</li>
                    <li>Set up automated monitoring for performance disparities</li>
                    <li>Create processes for model retraining when bias is detected</li>
                </ul>
            </div>
            
            <!-- SIMULATOR INSERTION POINT 4 -->
            <div class="simulator-placeholder">
                <h3>🧪 Interactive Exercise: Validation Strategy Exploration</h3>
                <p>Insert the Validation Strategy Simulator here to let students discover why external validation is "shockingly rare."</p>
                <p><strong>File:</strong> <span class="simulator-filename">Session_3_Validation_Strategy_Simulator_Ivey_Brand.html</span></p>
                <p><em>Students will configure data quality settings and experience how validation approaches reveal different performance truths.</em></p>
            </div>
        </div>

        <div class="reflection-questions">
            <h3>🤔 Business Application Questions</h3>
            <ol>
                <li><strong>Assessment:</strong> Think of an AI system your organization uses or is considering. What types of bias might affect it?</li>
                <li><strong>Stakeholder Analysis:</strong> Who would be affected by unfair outcomes from this system? What would fairness mean to each group?</li>
                <li><strong>Risk Evaluation:</strong> What would be the business, legal, and ethical consequences if this system exhibited bias?</li>
                <li><strong>Prevention Planning:</strong> At which stages of ML development could you intervene to prevent bias?</li>
                <li><strong>Monitoring Strategy:</strong> How would you detect bias after the system is deployed? What metrics would you track?</li>
            </ol>
        </div>

        <div class="conclusion-box">
            <h2>The Business Case for Bias Prevention</h2>
            <p>Understanding and preventing AI bias isn't just an ethical imperative—it's a business necessity. Organizations that fail to address bias face:</p>
            <ul>
                <li><strong>Legal liability:</strong> Growing regulations around algorithmic fairness</li>
                <li><strong>Reputation damage:</strong> Public exposure of biased systems can destroy trust</li>
                <li><strong>Performance costs:</strong> Biased systems perform poorly for underserved groups</li>
                <li><strong>Innovation limits:</strong> Bias reduces your ability to serve diverse markets</li>
            </ul>
            
            <p>The COMPAS case study provides a roadmap for what not to do, but more importantly, it offers a framework for systematic bias identification and prevention.</p>
            
            <div class="call-to-action">
                As you engage with the interactive components of this session, remember: <span class="highlight">Every technical choice is a values choice.</span> Your job as a business leader is to make those values explicit, involve stakeholders in defining fairness, and build systems that reflect your organization's commitment to equity and effectiveness.
            </div>
            
            <p>The framework you've learned—six development stages, nine bias types, competing fairness definitions—provides the foundation for responsible AI leadership. Use it to ask better questions, make more informed decisions, and build AI systems that serve all of your stakeholders effectively.</p>
        </div>
    </div>
</body>
</html>