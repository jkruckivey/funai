<!DOCTYPE html>
<html lang="en">
<head>
    <meta charset="UTF-8">
    <meta name="viewport" content="width=device-width, initial-scale=1.0">
    <title>COMPAS Case Study: Machine Learning Bias in the Courtroom</title>
    <style>
        * {
            margin: 0;
            padding: 0;
            box-sizing: border-box;
        }

        body {
            font-family: -apple-system, BlinkMacSystemFont, 'Segoe UI', 'Roboto', 'Oxygen', 'Ubuntu', 'Cantarell', sans-serif;
            background: #f5f5f5;
            color: #374151;
            line-height: 1.6;
            font-size: 16px;
            padding: 20px;
        }

        .container {
            max-width: 900px;
            margin: 0 auto;
            background: white;
            padding: 0;
        }

        h1 {
            font-size: 2rem;
            font-weight: 600;
            color: #1f2937;
            margin-bottom: 1.5rem;
        }

        h2 {
            font-size: 1.5rem;
            font-weight: 600;
            color: #1f2937;
            margin-bottom: 1rem;
            margin-top: 2rem;
        }

        h3 {
            font-size: 1.25rem;
            font-weight: 600;
            color: #1f2937;
            margin-bottom: 0.75rem;
            margin-top: 1.5rem;
        }

        p {
            color: #6b7280;
            margin-bottom: 1rem;
            line-height: 1.6;
        }

        /* Key Information Card - Uplift Style */
        .info-card {
            background: #2d5a5a;
            color: white;
            border-radius: 8px;
            padding: 20px;
            margin: 24px 0;
        }

        .info-card h3 {
            color: white;
            margin-top: 0;
            margin-bottom: 12px;
        }

        .info-card p {
            color: #a0c4c4;
            margin-bottom: 12px;
        }

        .info-card ul {
            color: #a0c4c4;
            padding-left: 20px;
        }

        .info-card li {
            margin-bottom: 8px;
        }

        /* Support Panel Style */
        .support-panel {
            background: #6b7280;
            color: white;
            border-radius: 8px;
            padding: 20px;
            margin: 24px 0;
            display: flex;
            gap: 12px;
        }

        .support-icon {
            width: 20px;
            height: 20px;
            background: #9ca3af;
            border-radius: 50%;
            display: flex;
            align-items: center;
            justify-content: center;
            color: white;
            font-size: 0.8rem;
            flex-shrink: 0;
            margin-top: 2px;
        }

        .support-content {
            flex: 1;
        }

        .support-title {
            font-weight: 600;
            font-size: 0.9rem;
            margin-bottom: 12px;
            text-transform: uppercase;
            letter-spacing: 0.5px;
        }

        /* Expandable Sections */
        .expandable-section {
            background: #e0f2fe;
            border: 1px solid #b3e5fc;
            border-radius: 8px;
            margin-bottom: 12px;
            overflow: hidden;
        }

        .section-header {
            padding: 16px 20px;
            background: #e0f2fe;
            cursor: pointer;
            display: flex;
            align-items: center;
            justify-content: space-between;
            transition: background-color 0.2s ease;
        }

        .section-header:hover {
            background: #b3e5fc;
        }

        .section-title {
            font-weight: 600;
            color: #1f2937;
            font-size: 1rem;
        }

        .chevron {
            width: 0;
            height: 0;
            border-left: 6px solid transparent;
            border-right: 6px solid transparent;
            border-top: 6px solid #6b7280;
            transition: transform 0.2s ease;
        }

        .section-header.expanded .chevron {
            transform: rotate(180deg);
        }

        .section-content {
            padding: 0 20px;
            max-height: 0;
            overflow: hidden;
            transition: max-height 0.3s ease, padding 0.3s ease;
        }

        .section-content.expanded {
            max-height: 1000px;
            padding: 16px 20px;
        }

        /* Debate Comparison */
        .debate-grid {
            display: grid;
            grid-template-columns: 1fr 1fr;
            gap: 20px;
            margin: 24px 0;
        }

        .debate-card {
            background: #e0f2fe;
            border: 1px solid #b3e5fc;
            border-radius: 8px;
            padding: 20px;
        }

        .debate-card h4 {
            color: #1f2937;
            font-weight: 600;
            margin-bottom: 12px;
            font-size: 1.1rem;
        }

        .debate-card .highlight {
            background: #2d5a5a;
            color: white;
            padding: 2px 6px;
            border-radius: 4px;
            font-weight: 600;
        }

        /* Statistics Table */
        .stats-table {
            width: 100%;
            border-collapse: collapse;
            margin: 20px 0;
            border-radius: 8px;
            overflow: hidden;
            border: 1px solid #e5e7eb;
        }

        .stats-table th {
            background: #f9fafb;
            color: #374151;
            font-weight: 600;
            padding: 12px;
            text-align: left;
            border-bottom: 1px solid #e5e7eb;
        }

        .stats-table td {
            padding: 12px;
            border-bottom: 1px solid #f3f4f6;
            color: #6b7280;
        }

        .stats-table tr:nth-child(even) {
            background: #f9fafb;
        }

        /* Reflection Questions */
        .reflection-section {
            background: #f9fafb;
            border: 1px solid #e5e7eb;
            border-radius: 8px;
            padding: 24px;
            margin: 24px 0;
        }

        .question-list {
            list-style: none;
            padding: 0;
            margin: 16px 0 0 0;
        }

        .question-item {
            background: white;
            border: 1px solid #e5e7eb;
            border-radius: 6px;
            padding: 16px;
            margin-bottom: 12px;
            color: #374151;
        }

        .question-number {
            color: #2d5a5a;
            font-weight: 700;
            margin-right: 8px;
        }

        /* Key Findings */
        .findings-grid {
            display: grid;
            grid-template-columns: repeat(auto-fit, minmax(280px, 1fr));
            gap: 16px;
            margin: 20px 0;
        }

        .finding-card {
            background: #f9fafb;
            border: 1px solid #e5e7eb;
            border-radius: 8px;
            padding: 16px;
            text-align: center;
        }

        .finding-number {
            background: #2d5a5a;
            color: white;
            border-radius: 50%;
            width: 32px;
            height: 32px;
            display: flex;
            align-items: center;
            justify-content: center;
            font-weight: 700;
            margin: 0 auto 8px;
            font-size: 0.9rem;
        }

        .finding-text {
            font-size: 0.9rem;
            color: #6b7280;
            line-height: 1.4;
        }

        /* Responsive Design */
        @media (max-width: 768px) {
            body {
                padding: 15px;
            }

            .debate-grid {
                grid-template-columns: 1fr;
                gap: 16px;
            }

            .findings-grid {
                grid-template-columns: 1fr;
            }

            .stats-table {
                font-size: 0.85rem;
            }

            .stats-table th,
            .stats-table td {
                padding: 8px;
            }

            h1 {
                font-size: 1.5rem;
            }

            h2 {
                font-size: 1.25rem;
            }
        }
    </style>
</head>
<body>
    <div class="container">
        <h1>COMPAS Case Study: Machine Learning Bias in the Courtroom</h1>

        <p>In 2016, a groundbreaking investigation by ProPublica sparked national debate about algorithmic bias in criminal justice. The controversy centered on COMPAS‚Äîa widely-used AI system for predicting recidivism‚Äîand raised fundamental questions about fairness, accuracy, and the role of algorithms in high-stakes decisions.</p>

        <div class="info-card">
            <h3>What is COMPAS?</h3>
            <p><strong>Correctional Offender Management Profiling for Alternative Sanctions (COMPAS)</strong> is a risk-assessment tool designed to predict the likelihood of defendant reoffending.</p>
            <ul>
                <li><strong>Developed:</strong> Northpointe Inc. (now Equivant), starting in 1998</li>
                <li><strong>Input:</strong> 137 variables including criminal history, demographics, and questionnaire responses</li>
                <li><strong>Output:</strong> Risk score from 1-10 (Low: 1-4, Medium: 5-7, High: 8-10)</li>
                <li><strong>Usage:</strong> Over 60,000 assessments annually in New York state by 2015</li>
            </ul>
        </div>

        <h2>The Debate: ProPublica vs. Northpointe</h2>

        <p>The controversy erupted when ProPublica published "Machine Bias," an investigation claiming COMPAS perpetuated racial discrimination. Northpointe strongly disputed these findings, leading to a debate that continues to shape how we think about algorithmic fairness.</p>

        <div class="debate-grid">
            <div class="debate-card">
                <h4>üîç ProPublica's Position</h4>
                <p><strong>Claim:</strong> COMPAS shows systematic bias against Black defendants</p>
                <p><strong>Evidence:</strong></p>
                <ul style="margin: 8px 0 0 20px; color: #6b7280;">
                    <li>Black defendants: <span class="highlight">44.9%</span> false positive rate</li>
                    <li>White defendants: <span class="highlight">23.5%</span> false positive rate</li>
                    <li>White defendants: <span class="highlight">47.7%</span> false negative rate</li>
                    <li>Black defendants: <span class="highlight">28%</span> false negative rate</li>
                </ul>
                <p style="margin-top: 12px;"><strong>Conclusion:</strong> Different error rates by race indicate systematic bias</p>
            </div>

            <div class="debate-card">
                <h4>‚öñÔ∏è Northpointe's Response</h4>
                <p><strong>Claim:</strong> COMPAS achieves predictive parity and accuracy equity</p>
                <p><strong>Counter-Evidence:</strong></p>
                <ul style="margin: 8px 0 0 20px; color: #6b7280;">
                    <li>Predictive parity: Equal accuracy within risk categories</li>
                    <li>Base rate differences: 51% vs 39% recidivism rates</li>
                    <li>Statistical measures (ROC) nearly identical by race</li>
                    <li>Algorithm uses objective, unbiased scoring rules</li>
                </ul>
                <p style="margin-top: 12px;"><strong>Conclusion:</strong> Differences reflect real population disparities, not bias</p>
            </div>
        </div>

        <div class="support-panel">
            <div class="support-icon">i</div>
            <div class="support-content">
                <div class="support-title">Key Insight: The Fairness Impossibility</div>
                <p>This debate reveals a fundamental mathematical truth: when base rates differ between groups, you cannot simultaneously achieve all types of fairness. Both ProPublica and Northpointe were technically correct‚Äîthey just prioritized different fairness definitions.</p>
            </div>
        </div>

        <h2>Statistical Evidence</h2>

        <p>Understanding the numbers behind the debate is crucial for evaluating the competing claims:</p>

        <table class="stats-table">
            <thead>
                <tr>
                    <th>Metric</th>
                    <th>Definition</th>
                    <th>Black Defendants</th>
                    <th>White Defendants</th>
                </tr>
            </thead>
            <tbody>
                <tr>
                    <td><strong>Base Rate</strong></td>
                    <td>Actual recidivism rate</td>
                    <td>51.0%</td>
                    <td>39.0%</td>
                </tr>
                <tr>
                    <td><strong>False Positive Rate</strong></td>
                    <td>Incorrectly labeled "high risk"</td>
                    <td>44.9%</td>
                    <td>23.5%</td>
                </tr>
                <tr>
                    <td><strong>False Negative Rate</strong></td>
                    <td>Incorrectly labeled "low risk"</td>
                    <td>28.0%</td>
                    <td>47.7%</td>
                </tr>
                <tr>
                    <td><strong>Positive Predictive Value</strong></td>
                    <td>Accuracy of "high risk" predictions</td>
                    <td>63.0%</td>
                    <td>59.0%</td>
                </tr>
            </tbody>
        </table>

        <h2>Bias Sources in COMPAS</h2>

        <p>Using our nine-type bias framework, we can identify multiple sources of bias throughout the COMPAS development process:</p>

        <div class="expandable-section">
            <div class="section-header" onclick="toggleSection(this)">
                <span class="section-title">Historical Bias: Past Inequities in Training Data</span>
                <div class="chevron"></div>
            </div>
            <div class="section-content">
                <p><strong>Source:</strong> COMPAS was trained on historical criminal justice data that reflects decades of systemic inequality, including over-policing of Black communities and disparate sentencing practices.</p>
                <p><strong>Impact:</strong> The algorithm learned to replicate these historical patterns, perpetuating past discrimination in future predictions.</p>
                <p><strong>Example:</strong> Higher arrest rates in over-policed communities become "evidence" of higher risk, creating a feedback loop.</p>
            </div>
        </div>

        <div class="expandable-section">
            <div class="section-header" onclick="toggleSection(this)">
                <span class="section-title">Representation Bias: Non-Representative Training Data</span>
                <div class="chevron"></div>
            </div>
            <div class="section-content">
                <p><strong>Source:</strong> Training data over-represented individuals from certain jurisdictions and communities, particularly those subject to intensive policing.</p>
                <p><strong>Impact:</strong> The model's understanding of "normal" behavior was skewed toward over-policed populations.</p>
                <p><strong>Example:</strong> Rural and suburban communities were underrepresented, making the model less accurate for these populations.</p>
            </div>
        </div>

        <div class="expandable-section">
            <div class="section-header" onclick="toggleSection(this)">
                <span class="section-title">Measurement Bias: Arrest Records as Proxy for Criminal Behavior</span>
                <div class="chevron"></div>
            </div>
            <div class="section-content">
                <p><strong>Source:</strong> COMPAS used arrest records rather than conviction records or actual criminal behavior as the basis for predictions.</p>
                <p><strong>Impact:</strong> Arrests can be influenced by police discretion, racial profiling, and resource allocation decisions unrelated to actual criminal activity.</p>
                <p><strong>Example:</strong> Drug use rates are similar across racial groups, but arrest rates vary dramatically due to policing practices.</p>
            </div>
        </div>

        <div class="expandable-section">
            <div class="section-header" onclick="toggleSection(this)">
                <span class="section-title">Aggregation Bias: One-Size-Fits-All Model</span>
                <div class="chevron"></div>
            </div>
            <div class="section-content">
                <p><strong>Source:</strong> COMPAS used a single model across different populations, jurisdictions, and contexts without accounting for local variations.</p>
                <p><strong>Impact:</strong> Model assumptions that worked for one population were inappropriately applied to others with different characteristics.</p>
                <p><strong>Example:</strong> Risk factors meaningful in urban settings might not apply in rural contexts, but the same model was used everywhere.</p>
            </div>
        </div>

        <div class="expandable-section">
            <div class="section-header" onclick="toggleSection(this)">
                <span class="section-title">Deployment Bias: Scope Expansion Beyond Training</span>
                <div class="chevron"></div>
            </div>
            <div class="section-content">
                <p><strong>Source:</strong> COMPAS was used for decisions beyond its original training scope, including bail determinations and sentencing when it was primarily trained for parole decisions.</p>
                <p><strong>Impact:</strong> The model was applied in contexts where its predictions might not be valid or appropriate.</p>
                <p><strong>Example:</strong> A model trained to predict long-term recidivism being used for short-term pretrial detention decisions.</p>
            </div>
        </div>

        <h2>Key Findings and Implications</h2>

        <div class="findings-grid">
            <div class="finding-card">
                <div class="finding-number">1</div>
                <div class="finding-text"><strong>Multiple Bias Sources:</strong> COMPAS exhibits bias from historical data, measurement choices, and deployment practices</div>
            </div>

            <div class="finding-card">
                <div class="finding-number">2</div>
                <div class="finding-text"><strong>Proxy Variables:</strong> Excluding race explicitly doesn't prevent bias when other variables serve as proxies</div>
            </div>

            <div class="finding-card">
                <div class="finding-number">3</div>
                <div class="finding-text"><strong>Fairness Trade-offs:</strong> Different fairness definitions lead to different conclusions about the same system</div>
            </div>

            <div class="finding-card">
                <div class="finding-number">4</div>
                <div class="finding-text"><strong>Transparency Issues:</strong> Proprietary algorithms make bias detection and accountability difficult</div>
            </div>

            <div class="finding-card">
                <div class="finding-number">5</div>
                <div class="finding-text"><strong>Systemic Impact:</strong> Individual-level bias aggregates to reinforce societal inequalities</div>
            </div>

            <div class="finding-card">
                <div class="finding-number">6</div>
                <div class="finding-text"><strong>Stakeholder Responsibility:</strong> Both developers and users share accountability for fair outcomes</div>
            </div>
        </div>

        <h2>Critical Reflection Questions</h2>

        <div class="reflection-section">
            <h3 style="color: #2d5a5a; margin-top: 0;">Consider These Questions</h3>
            <p>As you analyze the COMPAS case, reflect on these fundamental issues:</p>

            <ol class="question-list">
                <li class="question-item">
                    <span class="question-number">1.</span>
                    <strong>Taking Sides:</strong> Do you find ProPublica's or Northpointe's argument more compelling? What evidence supports your position?
                </li>

                <li class="question-item">
                    <span class="question-number">2.</span>
                    <strong>Fairness Definitions:</strong> How do different definitions of "fairness" lead to different conclusions about the same algorithm?
                </li>

                <li class="question-item">
                    <span class="question-number">3.</span>
                    <strong>Bias Mitigation:</strong> If you were managing COMPAS development, what steps would you take to address the identified bias sources?
                </li>

                <li class="question-item">
                    <span class="question-number">4.</span>
                    <strong>Business Applications:</strong> How do the lessons from COMPAS apply to AI systems in hiring, lending, or healthcare?
                </li>

                <li class="question-item">
                    <span class="question-number">5.</span>
                    <strong>Systemic Issues:</strong> Should AI systems mirror existing societal patterns or actively work to counteract historical inequities?
                </li>

                <li class="question-item">
                    <span class="question-number">6.</span>
                    <strong>Accountability:</strong> Who bears responsibility when algorithmic bias causes harm‚Äîdevelopers, users, or society?
                </li>
            </ol>
        </div>

        <div class="info-card">
            <h3>Beyond COMPAS: Broader Implications</h3>
            <p>The COMPAS controversy extends far beyond criminal justice. Similar bias challenges appear in:</p>
            <ul>
                <li><strong>Hiring Systems:</strong> Resume screening and candidate evaluation algorithms</li>
                <li><strong>Credit Scoring:</strong> Loan approval and interest rate determination</li>
                <li><strong>Healthcare AI:</strong> Treatment recommendations and resource allocation</li>
                <li><strong>Content Curation:</strong> News feeds and recommendation systems</li>
            </ul>
            <p>Understanding COMPAS provides a framework for identifying and addressing bias in any high-stakes AI application.</p>
        </div>
    </div>

    <script>
        function toggleSection(header) {
            const content = header.nextElementSibling;
            const chevron = header.querySelector('.chevron');

            if (content.classList.contains('expanded')) {
                content.classList.remove('expanded');
                header.classList.remove('expanded');
            } else {
                content.classList.add('expanded');
                header.classList.add('expanded');
            }
        }
    </script>
</body>
</html>